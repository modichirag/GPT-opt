# config.yaml
optimizer_params:
  - name: adam
    lr: [0.0001, 0.0005, 0.001]
    weight_decay: 0
    lr_schedule: constant

  - name: momo-adam
    lr: [0.0001, 0.0005, 0.001]
    weight_decay: 0
    lb: 2.5
    lr_schedule: constant

  # - name: sgd-m
  #   lr: [0.00005,  0.0001,  0.0005]
  #   weight_decay: 0
  #   momentum: 0.9
  #   dampening: 0.9
  #   lr_schedule: warm-up-cosine
  #   warm_up_percent: 0.2

  - name: muon
    lr: [0.0001, 0.0005, 0.001]
    weight_decay: 0
    lr_schedule: constant

training_params:
  tokens_processed: 262144 # 2^18 #131072 # 2^17 # 524288 # 2^19
  val_tokens_processed: 8388608 #2^23
  batch_size: 32
  num_epochs: 1
  context_length: 1024
  gradnorm: 1.0
  tensorcore_precision: high   #Can be highest, high, or medium
  autocast: True
  mixed_precision: bfloat16
  compile: True

logging_params:
  val_tokens_processed: 8388608 #2^23
  log_step: 20
  val_step: 50
  save_ckpt_step: 100
  load_ckpt_step: 700

gpt_model:
  model_name: "gpt2"  # If specified, it ignores explicit parameters specified below and loads this model name. 
  pretrained: False
  tokenizer: "gpt2"
  flash_attention: True
  # n_embd: 768    # Hidden size used in gpt2
  # n_layer: 2    # Number of layers in gpt2
  # n_head: 4    # Number of attention heads in gpt2
  # vocab_size: 50304

dataset:
  name: "finewebmini"