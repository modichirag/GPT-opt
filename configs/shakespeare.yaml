# config.yaml

name: "tiny_shakespeare"

optimizer_params:
  - name: adam
    lr: [0.00005, 0.0001, 0.0005]
    weight_decay: 0
    lr_schedule: constant

  - name: momo-adam
    lr: [0.00005, 0.0001, 0.0005]
    weight_decay: 0
    lb: 2.5
    lr_schedule: constant

  - name: sgd-m
    lr: [0.00005,  0.0001,  0.0005]
    weight_decay: 0
    momentum: 0.9
    dampening: 0.9
    lr_schedule: warm-up-cosine
    warm_up_percent: 0.2

training_params:
  batch_size: 8
  num_epochs: 1
  max_length: 512

gpt_model:
  model_name: "gpt2-medium"  #  You can use one of the pre-defined models of transformers, or you can specify the exact dimension below
  n_embd: 768    # Hidden size used in gpt2
  n_layer: 2    # Number of layers in gpt2
  n_head: 4    # Number of attention heads in gpt2
  vocab_size: 50304
  tokenizer_name: "gpt2-medium"

dataset:
  name: "tiny_shakespeare"  